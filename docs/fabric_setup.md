# ğŸš€ Guide de DÃ©ploiement - Risk, Compliance & Audit Analytics sur Microsoft Fabric

## ğŸ“‹ PrÃ©requis

- **Microsoft Fabric** : CapacitÃ© active (F64 minimum recommandÃ© pour AI features)
- **Workspace Fabric** : Droit Contributor minimum
- **Python 3.9+** : Pour gÃ©nÃ©ration de donnÃ©es (local)
- **Power BI Desktop** : (optionnel) pour tester Semantic Model localement

---

## ğŸ—ï¸ Architecture Cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FABRIC WORKSPACE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  LAKEHOUSE   â”‚â”€â”€â”€â”€â”€â”€â”‚   SEMANTIC MODEL    â”‚        â”‚
â”‚  â”‚              â”‚      â”‚                     â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  âœ“ Relations        â”‚        â”‚
â”‚  â”‚  â”‚ Bronze â”‚  â”‚      â”‚  âœ“ DAX Measures     â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚      â”‚  âœ“ RLS (optionnel)  â”‚        â”‚
â”‚  â”‚       â”‚      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”  â”‚                 â”‚                   â”‚
â”‚  â”‚  â”‚ Silver â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚      â”‚    DATA AGENT       â”‚        â”‚
â”‚  â”‚       â”‚      â”‚      â”‚                     â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”  â”‚      â”‚  Persona: CCO       â”‚        â”‚
â”‚  â”‚  â”‚  Gold  â”‚  â”‚      â”‚  + AI Shortcut      â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚    (Text Analysis)  â”‚        â”‚
â”‚  â”‚              â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚  â”‚  AI Shortcut    â”‚                               â”‚
â”‚  â”‚  â”‚  audit_reports_ â”‚                               â”‚
â”‚  â”‚  â”‚  txt/           â”‚                               â”‚
â”‚  â”‚  â”‚  incident_desc_ â”‚                               â”‚
â”‚  â”‚  â”‚  txt/           â”‚                               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Ã‰tape 1 : GÃ©nÃ©ration des DonnÃ©es (Local)

### 1.1 Cloner le Repository

```powershell
cd C:\Dev
git clone <your-repo-url> MF_RiskComplianceAudit
cd MF_RiskComplianceAudit
```

### 1.2 Installer les DÃ©pendances

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

**requirements.txt :**
```
Faker==22.6.0
pandas==2.2.0
numpy==1.26.3
PyYAML==6.0.1
```

### 1.3 GÃ©nÃ©rer les DonnÃ©es

```powershell
python src/generate_data.py
```

**Sortie attendue :**
```
ğŸš€ Starting Risk, Compliance & Audit data generation...

âœ… Generated 149 controls
âœ… Generated 34091 control executions
âœ… Generated 200 incidents
âœ… Generated 186 remediation actions
âœ… Generated 100 vendors
âœ… Generated 100 audit reports
âœ… Generated 150 incident descriptions

ğŸ“Š Summary:
   - Controls: 149
   - Control Executions: 34091
   - Incidents: 200
   - Remediation Actions: 186
   - Vendors: 100
   - Audit Reports: 100
   - Incident Descriptions: 150

ğŸ’¾ Data saved to: data/raw/
```

### 1.4 Valider les DonnÃ©es

```powershell
python src/validate_schema.py
```

**Sortie attendue :**
```
ğŸ” Starting Risk, Compliance & Audit data validation...

âœ… controls.csv: 7 columns, 149 rows
âœ… control_executions.csv: 6 columns, 34091 rows
âœ… incidents.csv: 8 columns, 200 rows
âœ… remediation_actions.csv: 9 columns, 186 rows
âœ… vendors.csv: 7 columns, 100 rows
âœ… Relationship control_executions â†’ controls: OK
âœ… Relationship incidents â†’ control_executions: OK
âœ… Relationship incidents â†’ vendors: OK
âœ… Relationship remediation_actions â†’ incidents: OK
âœ… audit_reports_txt/: 100 files
âœ… incident_descriptions_txt/: 150 files
âœ… Compliance Rate: 69.9% (realistic)

ğŸ‰ All validations passed!
```

---

## â˜ï¸ Ã‰tape 2 : CrÃ©er le Lakehouse dans Fabric

### 2.1 CrÃ©er le Workspace

1. Ouvrir **Microsoft Fabric** ([https://app.fabric.microsoft.com](https://app.fabric.microsoft.com))
2. Cliquer **Workspaces** â†’ **+ New workspace**
3. Nom : `Risk_Compliance_Analytics`
4. Licence : F64 ou supÃ©rieur (pour AI features)
5. Cliquer **Apply**

### 2.2 CrÃ©er le Lakehouse

1. Dans le workspace, cliquer **+ New** â†’ **Lakehouse**
2. Nom : `Compliance_Lakehouse`
3. Cliquer **Create**

### 2.3 CrÃ©er la Structure de Dossiers

Dans le Lakehouse Explorer (Files) :
```
Files/
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ controls.csv
â”‚   â”œâ”€â”€ control_executions.csv
â”‚   â”œâ”€â”€ incidents.csv
â”‚   â”œâ”€â”€ remediation_actions.csv
â”‚   â””â”€â”€ vendors.csv
â”œâ”€â”€ audit_reports_txt/
â”‚   â””â”€â”€ (100 fichiers .txt)
â””â”€â”€ incident_descriptions_txt/
    â””â”€â”€ (150 fichiers .txt)
```

---

## ğŸ“¤ Ã‰tape 3 : Upload des DonnÃ©es

### 3.1 Upload via OneLake File Explorer (RecommandÃ©)

1. Installer **OneLake File Explorer** depuis Microsoft Store
2. Ouvrir OneLake File Explorer
3. Naviguer vers `Risk_Compliance_Analytics/Compliance_Lakehouse/Files`
4. Glisser-dÃ©poser :
   - `data/raw/*.csv` â†’ `bronze/`
   - `data/raw/audit_reports_txt/` â†’ `audit_reports_txt/`
   - `data/raw/incident_descriptions_txt/` â†’ `incident_descriptions_txt/`

### 3.2 Upload via Interface Web (Alternatif)

1. Dans Lakehouse, cliquer **Upload** â†’ **Upload folder**
2. SÃ©lectionner `data/raw/` localement
3. Attendre fin upload (peut prendre 3-5 min pour 34K rows + 250 files)

---

## ğŸ”„ Ã‰tape 4 : Transformation Bronze â†’ Silver â†’ Gold

### 4.1 Importer le Notebook Bronze â†’ Silver

Les notebooks de transformation sont dÃ©jÃ  crÃ©Ã©s dans le dossier `notebooks/` du projet.

1. Dans le workspace Fabric, cliquer **Import** â†’ **Notebook**
2. SÃ©lectionner le fichier `notebooks/01_bronze_to_silver.ipynb`
3. Attendre l'import (quelques secondes)
4. Ouvrir le notebook importÃ©
5. **ExÃ©cuter toutes les cellules** (bouton **Run all** ou Ctrl+Shift+Enter sur chaque cellule)

**Ce que fait le notebook :**
- âœ… Charge les 5 fichiers CSV depuis Files/bronze/
- âœ… Applique les transformations (typage des dates, nettoyage)
- âœ… CrÃ©e les tables Delta Silver dans le Lakehouse
- âœ… Affiche un aperÃ§u de validation

**VÃ©rification :** AprÃ¨s exÃ©cution, vÃ©rifiez que les 5 tables apparaissent dans la section **Tables** du Lakehouse :
- controls
- control_executions
- incidents
- remediation_actions
- vendors

### 4.2 Importer le Notebook Silver â†’ Gold

1. Dans le workspace Fabric, cliquer **Import** â†’ **Notebook**
2. SÃ©lectionner le fichier `notebooks/02_silver_to_gold.ipynb`
3. Attendre l'import
4. Ouvrir le notebook importÃ©
5. **ExÃ©cuter toutes les cellules**

**Ce que fait le notebook :**
- âœ… Charge les tables Silver depuis le Lakehouse
- âœ… CrÃ©e 4 tables Gold (agrÃ©gations mÃ©tier) :
  - **gold_framework_metrics** : MÃ©triques de conformitÃ© par framework
  - **gold_incident_metrics** : Analyse incidents par type/sÃ©vÃ©ritÃ©
  - **gold_vendor_risk** : Analyse de risque vendors
  - **gold_remediation_metrics** : Performance des actions correctives
- âœ… Affiche un rÃ©sumÃ© des tables crÃ©Ã©es

**VÃ©rification :** Les 4 tables Gold doivent apparaÃ®tre dans la section **Tables** du Lakehouse

---

## ğŸ”— Ã‰tape 5 : CrÃ©er le Semantic Model

### 5.1 CrÃ©er Semantic Model depuis Lakehouse

1. Dans Lakehouse, cliquer **New Semantic Model**
2. Nom : `Compliance_Model`
3. SÃ©lectionner tables :
   - âœ… controls
   - âœ… control_executions
   - âœ… incidents
   - âœ… remediation_actions
   - âœ… vendors
4. Cliquer **Confirm**

### 5.2 DÃ©finir les Relations

Ouvrir `Compliance_Model` â†’ **Model view**

**CrÃ©er relations :**
```
controls[control_id] â”€â”€(1)â”€â”€(N)â”€â”€ control_executions[control_id]
control_executions[execution_id] â”€â”€(1)â”€â”€(N)â”€â”€ incidents[execution_id]
vendors[vendor_id] â”€â”€(1)â”€â”€(N)â”€â”€ incidents[vendor_id]
incidents[incident_id] â”€â”€(1)â”€â”€(N)â”€â”€ remediation_actions[incident_id]
```

**ParamÃ¨tres relations :**
- CardinalitÃ© : Many-to-One
- Cross-filter direction : Both (bidirectional)

### 5.3 Ajouter Mesures DAX

Dans **Report view** â†’ Cliquer table `control_executions` â†’ **New measure**

Copier-coller les mesures depuis `docs/dax_measures.md` (30+ mesures disponibles)

**Mesures essentielles :**
```dax
Compliance Rate = 
DIVIDE(
    CALCULATE(COUNTROWS(control_executions), control_executions[status] = "passed"),
    COUNTROWS(control_executions),
    0
)

Open Incidents = 
CALCULATE(
    COUNTROWS(incidents),
    incidents[status] IN {"open", "investigating"}
)

MTTR = 
AVERAGEX(
    remediation_actions,
    DATEDIFF(
        remediation_actions[start_date],
        remediation_actions[completion_date],
        DAY
    )
)
```

---

## ğŸ¤– Ã‰tape 6 : Configurer AI Shortcut (Texte)

### 6.1 CrÃ©er AI Shortcut pour Audit Reports

1. Dans Lakehouse, cliquer **Get data** â†’ **AI Shortcut**
2. Source : `Files/audit_reports_txt/`
3. Nom : `AI_Audit_Reports`
4. Settings :
   - Enable semantic chunking : âœ…
   - Chunk size : 500 tokens
   - Overlap : 50 tokens
5. Cliquer **Create**

### 6.2 CrÃ©er AI Shortcut pour Incident Descriptions

1. RÃ©pÃ©ter pour `Files/incident_descriptions_txt/`
2. Nom : `AI_Incident_Descriptions`
3. MÃªmes settings

**â±ï¸ Temps d'indexation :** 3-5 minutes pour 250 fichiers

---

## ğŸ§  Ã‰tape 7 : CrÃ©er et Configurer le Data Agent

### 7.1 CrÃ©er Data Agent

1. Dans workspace, cliquer **+ New** â†’ **Data Agent**
2. Nom : `CCO_Compliance_Agent`
3. Data sources :
   - âœ… Compliance_Model (Semantic Model)
   - âœ… AI_Audit_Reports (AI Shortcut)
   - âœ… AI_Incident_Descriptions (AI Shortcut)
4. Cliquer **Create**

### 7.2 Configurer Instructions SystÃ¨me

Copier le contenu de `docs/data_agent_instructions.md` dans **System Instructions**

**Snippet clÃ© :**
```
You are David Laurent, Chief Compliance Officer at FinSecure Corp.
Your role is to ensure compliance across SOX, GDPR, ISO27001, and PCI-DSS frameworks.

Key Metrics:
- Compliance Rate > 90%
- Incident Response Time: Critical <24h, High <72h
- Mean Time To Remediate (MTTR) < 7 days
- Vendor Risk Score < 70

When asked about compliance, ALWAYS:
1. Check control_executions for compliance rate
2. Identify failed controls by framework
3. Analyze incidents linked to control failures
4. Extract insights from audit_reports if available
```

### 7.3 Tester le Data Agent

Poser questions de `docs/questions_demo.md` pour valider setup :

**Test 1 :**
> "Quel est le taux de conformitÃ© global ?"

**RÃ©ponse attendue :** 69.9% (vs target 90%)

**Test 2 :**
> "Extrais des audit reports les recommandations rÃ©currentes pour SOX"

**RÃ©ponse attendue :** Extraction texte mentionnant "automated alerts" (23Ã—)

---

## âœ… Ã‰tape 8 : Validation Finale

### 8.1 Checklist de Validation

- [ ] 5 tables Delta crÃ©Ã©es dans Lakehouse
- [ ] Relations correctement dÃ©finies dans Semantic Model
- [ ] 10+ mesures DAX disponibles
- [ ] 2 AI Shortcuts indexÃ©s (audit_reports + incident_descriptions)
- [ ] Data Agent rÃ©pond correctement aux 3 premiÃ¨res questions de `questions_demo.md`
- [ ] Compliance Rate calculÃ© = ~69.9% (cohÃ©rent avec donnÃ©es gÃ©nÃ©rÃ©es)
- [ ] Framework breakdown : SOX 68.5%, GDPR 71.2%, ISO27001 70.1%, PCI-DSS 69.8%

### 8.2 Test de Performance

Poser la question complexe :
> "Cherche dans incident descriptions toute mention de 'offboarding' et identifie le contrÃ´le qui devrait dÃ©tecter ces incidents"

**Temps de rÃ©ponse attendu :** < 30 secondes  
**QualitÃ© rÃ©ponse :** Doit mentionner 18 incidents + CTRL_023 (Access Review) avec 58% compliance

---

## ğŸ“ Ã‰tape 9 : DÃ©mo Ready !

Votre environnement est maintenant prÃªt pour :
- âœ… DÃ©montrer Data Agent avec `questions_demo.md`
- âœ… Raconter story `demo_story.md` ("L'Audit qui RÃ©vÃ¨le")
- âœ… Montrer AI extraction de texte
- âœ… Calculer ROI automation (compliance 69.9% â†’ 81.2%)

---

## ğŸ› Troubleshooting

### ProblÃ¨me : AI Shortcut ne retourne pas de rÃ©sultats

**Solution :**
- VÃ©rifier que fichiers .txt sont bien dans `Files/audit_reports_txt/` (pas dans Tables)
- Attendre fin d'indexation (Check **Settings** â†’ **Indexing status**)
- Re-crÃ©er AI Shortcut si nÃ©cessaire

### ProblÃ¨me : Relations ne se crÃ©ent pas automatiquement

**Solution :**
- VÃ©rifier noms de colonnes EXACT (control_id, execution_id, etc.)
- CrÃ©er relations manuellement dans Model view
- VÃ©rifier types de donnÃ©es (string pour IDs)

### ProblÃ¨me : Mesures DAX retournent 0 ou BLANK

**Solution :**
- VÃ©rifier relations bidirectionnelles activÃ©es
- VÃ©rifier filtres actifs dans rapport
- Tester mesure simple d'abord : `Total Executions = COUNTROWS(control_executions)`

---

## ğŸ“š Ressources

- **Documentation Fabric :** [https://learn.microsoft.com/fabric](https://learn.microsoft.com/fabric)
- **DAX Guide :** [https://dax.guide](https://dax.guide)
- **Data Agent Docs :** [https://learn.microsoft.com/fabric/data-agent](https://learn.microsoft.com/fabric/data-agent)

---

**Auteur :** Microsoft Fabric Demo Team  
**Version :** 1.0  
**Temps d'installation :** ~60 minutes  
**Date :** FÃ©vrier 2026
